

许多概率模型是很难训练的，其根本原因是很难进行推断。
在\gls{DL}中，我们通常有一系列的可见变量$\Vv$和一系列的隐含变量$\Vh$。
推断的挑战往往在于计算$P(\Vh\vert\Vv)$或者计算在$P(\Vh\vert\Vv)$下的期望的困难性。
这样的操作在一些任务比如\gls{MLE}中往往又是必须的。
% 623

许多诸如\gls{RBM}和\gls{PPCA}这样的仅仅含有一层隐层的简单的\gls{graphical_models}的定义，往往使得推断操作如计算$P(\Vh\vert\Vv)$或者计算$P(\Vh\vert\Vv)$下的期望是非常容易的。
不幸的是，大多数的具有多层隐藏变量的\gls{graphical_models}的后验分布都很难处理。
精确的推断算法需要一个指数量级的运行时间。
即使一些只有单层的模型，如\gls{sparse_coding}，也存在着这样的问题。
% 623


在本章中，我们介绍了几个基本的技巧，用来解决难以处理的推断问题。
稍后，在第\ref{chap:deep_generative_models}章中，我们还将描述如何将这些技巧应用到训练其他方法难以奏效的概率模型中，如\gls{DBN}，\gls{DBM}。
% 623


在\gls{DL}中难以处理的推断问题通常源于\gls{structured_probabilistic_models}中的隐含变量之间的相互作用。
详见图\ref{fig:intractable_graphs}的几个例子。
这些相互作用可能是\gls{undirected_model}的直接作用，也可能是\gls{directed_model}中的一个可见变量的共同祖先之间的\gls{explaining_away}作用。
% 623 end


\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
	\centerline{\includegraphics[width=0.8\textwidth]{Chapter19/figures/intractable_graphs}}
\fi
	\caption{TODO}
	\label{fig:intractable_graphs}
\end{figure}


\section{推断是一个优化问题}
\label{sec:inference_as_optimization}
% 624

许多难以利用观察值进行精确推断的问题往往可以描述为是一个优化问题。
通过近似这样一个潜在的优化问题，我们往往可以推导出近似推断算法。
% 624


为了构造这样一个优化问题，假设我们有一个包含可见变量$\Vv$和隐含变量$\Vh$的概率模型。
我们希望计算观察数据的概率的对数$\log p(\Vv;\Vtheta)$。
有时候如果消去$\Vh$来计算$\Vx$的边缘分布很费时的话，我们通常很难计算$\log p(\Vv;\Vtheta)$。
作为替代，我们可以计算一个$\log p(\Vv;\Vtheta)$的下界。
这个下界叫做\firstall{ELBO}。
这个下界的另一个常用的名字是负的\firstgls{variational_free_energy}。
这个\glssymbol{ELBO}是这样定义的：

\begin{align}
\CalL(\Vv,{\Vtheta},q) = \log p(\Vv;{\Vtheta}) - D_{\text{KL}}(q(\Vh\vert\Vv) \Vert p(\Vh\vert\Vv;{\Vtheta}))
\end{align}
在这里 $q$是关于$\Vh$的一个任意的概率分布。
% 625


因为$\log p(\Vv)$和$\CalL(\Vv,{\Vtheta},q)$之间的距离是由\gls{KL}来衡量的。
因为\gls{KL}总是非负的，我们可以发现$\CalL$小于等于所求的概率的对数。
当且仅当$q$完全相等于$p(\Vh\vert\Vv)$时取到等号。
% 625


令人吃惊的是，对某些分布$q$，$\CalL$可以被化的更简单。
通过简单的代数运算我们可以把$\CalL$写成更加简单的形式：

\begin{align}
\CalL(\Vv,{\Vtheta},q) = & \log p(\Vv;{\Vtheta})- D_{\text{KL}}(q(\Vh\vert\Vv)\Vert p(\Vh\vert\Vv;{\Vtheta})) \\
= & \log p(\Vv;{\Vtheta}) - \SetE_{\Vh\sim q}\log \frac{q(\Vh\vert\Vv)}{p(\Vh\vert\Vv)} \\
= & \log p(\Vv;{\Vtheta}) -  \SetE_{\Vh\sim q} \log \frac{q(\Vh\vert\Vv) }{ \frac{p(\Vh,\Vv;{\Vtheta})}{p(\Vv; {\Vtheta})} } \\
= & \log p(\Vv; {\Vtheta}) -  \SetE_{\Vh\sim q} [\log q(\Vh\vert\Vv) - \log {p(\Vh,\Vv; {\Vtheta})} + \log {p(\Vv; {\Vtheta})} ]\\
= & - \SetE_{\Vh\sim q}[\log q(\Vh\vert\Vv) - \log {p(\Vh,\Vv;{\Vtheta})}]
\end{align}
% 625


这也给出了\glssymbol{ELBO}的标准的定义：
\begin{align}
\CalL(\Vv,{\Vtheta},q) = \SetE_{\Vh\sim q}[\log q(\Vh , \Vv)] + H(q)
\end{align}


对于一个较好的选择$q$来说，$\CalL$是可以计算的。
对任意选择$q$来说，$\CalL$提供了一个似然函数的下界。
越好的近似$q$的分布$q(\Vh\vert\Vv)$得到的下界就越紧，即与$\log p(\Vv)$更加接近。
当$q(\Vh\vert\Vv) = p(\Vh\vert\Vv)$时，这个近似是完美的，也意味着$\CalL(\Vv,{\Vtheta},q) = \log {p(\Vv;{\Vtheta})} $。
% 625


我们可以将推断问题看做是找一个分布$q$使得$\CalL$最大的过程。
精确的推断能够通过找一组包含分布$p(\Vh\vert\Vv)$的函数，完美的最大化$\CalL$。
在本章中，我们将会讲到如何通过近似优化来找$q$的方法来推导出不同形式的的近似推断。
我们可以通过限定分布$q$的形式或者使用并不彻底的优化方法来使得优化的过程更加高效，但是优化的结果是不完美的，因为只能显著的提升$\CalL$而无法彻底的最大化$\CalL$。
% 625  end


无论什么样的$q$的选择，$\CalL$是一个下界。
我们可以通过选择一个更简单抑或更复杂的计算过程来得到对应的更加松的或者更紧的下界。
通过一个不彻底的优化过程或者将$q$分布做很强的限定（使用一个彻底的优化过程）我们可以获得一个很差的$q$，尽管计算开销是极小的。
% 626 head


\section{\gls{EM}}
\label{sec:expectation_maximization}
% 626

我们介绍的第一个最大化下界$\CalL$的算法是\firstall{EM}。
在隐变量模型中，这是一个非常热门的训练算法。
在这里我们描述\citet{emview}所提出的\glssymbol{EM}算法。
不像大多数我们在本章中介绍的其他算法一样，\glssymbol{EM}并不是一个近似推断算法，但是是一种能够学到近似后验的算法。
% 626


\glssymbol{EM}算法包含了交替的两步运算直到收敛的过程：
\begin{itemize}
	\item \firstgls{e_step}: 令${\Vtheta^{(0)}}$表示在这一步开始时参数的初始值。
	令$q(\Vh^{(i)}\vert \Vv) = p(\Vh^{(i)}\vert\Vv^{(i)};\Vtheta^{(0)})$对任何我们想要训练的(对所有的或者小批数据均成立)索引为$i$的训练样本$\Vv^{(i)}$。
	通过这个定义，我们认为$q$在当前的参数$\Vtheta^{(0)}$下定义。
	如果我们改变$\Vtheta$，那么$p(\Vh\vert\Vv;\Vtheta)$将会相应的变化，但是$q(\Vh\vert\Vv)$还是不变并且等于$p(\Vh\vert\Vv;\Vtheta^{(0)})$。
	\item \firstgls{m_step}：使用选择的优化算法完全地或者部分的最大化关于$\Vtheta$的
	\begin{align}
	\sum_i \CalL(\Vv^{(i)},\Vtheta,q)
	\end{align}
\end{itemize}
% 626  


这可以被看做是一个坐标上升算法来最大化$\CalL$。
在第一步中，我们更新$q$来最大化$\CalL$，而另一步中，我们更新$\Vtheta$来最大化$\CalL$。
% 626


基于隐变量模型的\gls{SGA}可以被看做是一个\glssymbol{EM}算法的特例，其中\gls{m_step}包括了单次梯度的操作。
\glssymbol{EM}算法的其他变种可以实现多次梯度操作。
对一些模型，\gls{m_step}可以通过推出理论解直接完成，不同于其他方法，在给定当前$q$的情况下直接求出最优解。
% 626 end


即使\gls{e_step}采用的是精确推断，我们仍然可以将\glssymbol{EM}算法视作是某种程度上的近似推断。
具体地说，\gls{m_step}假设了一个$q$分布可以被所有的$\Vtheta$分享。
当\gls{m_step}越来越远离\gls{e_step}中的$\Vtheta^{(0)}$的时候，这将会导致$\CalL$和真实的$\log p(\Vv)$的差距。
此外，当下一个循环的时候，\gls{e_step}把这种差距又降到了0。
% 627 head



\glssymbol{EM}算法包含了一些不同的解释。
首先，学习过程中的基本结构中，我们通过更新模型参数来提高整个数据集的似然，其中缺失的变量的值是通过后验分布来估计的。
这种解释并不仅仅适用于\glssymbol{EM}算法。
比如说，使用梯度下降来最大化似然函数的对数这种方法也利用了相同的性质。
计算对数似然函数的梯度需要对隐含节点的后验分布来求期望。 
\glssymbol{EM}算法的另一个关键的性质是当我们移动到另一个$\Vtheta$时候，我们仍然可以使用旧的$q$。
在传统\gls{ML}中，这种特有的性质在推导M步更新时候被广泛的应用。
在\gls{DL}中，大多数模型太过于复杂以致于在M步中很难得到一个最优解。
所以\glssymbol{EM}算法的第二个特质较少的被使用。
% 627


\section{\gls{MAP}推断和\gls{sparse_coding}}
\label{sec:map_inference_and_sparse_coding}
% 627


我们通常使用\firstgls{inference}这个术语来指代给定一定条件下计算一系列变量的概率分布的过程。
当训练带有隐含变量的概率模型的时候，我们通常关注于计算$p(\Vh\vert\Vv)$。
在推断中另一个选择是计算一个最有可能的隐含变量的值来代替在其完整分布上的抽样。
在隐含变量模型中，这意味着计算
\begin{align}
\Vh^* = \underset{\Vh}{\arg\max} \ \  p(\Vh\vert\Vv)
\end{align}
这被称作是\firstall{MAP}推断。
% 627  



\gls{MAP}推断并不是一种近似推断，它计算了最有可能的一个$\Vh^*$。
然而，如果我们希望能够最大化$\CalL(\Vv,\Vh,q)$，那么我们可以把\gls{MAP}推断看成是输出一个$q$的学习过程。%是很有帮助的。
在这种情况下，我们可以将\gls{MAP}推断看成是近似推断，因为它并不能提供一个最优的$q$。
% 627 end



我们回过头来看看\ref{sec:inference_as_optimization}节中所描述的精确推断，它指的是关于一个不受限的$q$分布使用精确的优化算法来最大化
\begin{align}
\CalL(\Vv,{\Vtheta},q)
 = \SetE_{\Vh\sim q}[\log q(\Vh , \Vv)] + H(q).
\end{align}
我们通过限定$q$分布属于某个分布族，能够使得\gls{MAP}推断成为一种形式的近似推断。
具体的说，我们令$q$分布满足一个\gls{dirac_distribution}：
\begin{align}
q(\Vh\vert\Vv) = \delta(\Vh - {\Vmu})
\end{align}
这也意味着我们可以通过$\Vmu$来完全控制$q$。
通过将$\CalL$中不随$\Vmu$变化的项丢弃，剩下的我们遇到的是一个优化问题：
\begin{align}
\Vmu^*  =  \underset{\Vmu}{\arg\max}\ \log p(\Vh = \Vmu,\Vv)
\end{align}
这等价于\gls{MAP}推断问题
\begin{align}
\Vh^* = \underset{\Vh}{\arg\max}\  p(\Vh\vert\Vv)
\end{align}
% 628




因此我们能够解释一种类似于\glssymbol{EM}算法的学习算法，其中我们轮流迭代两步，一步是用\gls{MAP}推断估计出$\Vh^*$，另一步是更新$\Vtheta$来增大$\log p(\Vh^*,\Vv)$。
从\glssymbol{EM}算法角度看，这也是一种形式的对$\CalL$的坐标上升，\glssymbol{EM}算法的坐标上升中，交替迭代的时候通过推断来优化$\CalL$关于$q$以及通过参数更新来优化$\CalL$关于$\Vtheta$。
整体上说，这个算法的正确性可以被保证因为$\CalL$是$\log p(\Vv)$的下界。
在\gls{MAP}推断中，这个保证是无效的，因为这个界会无限的松，由于\gls{dirac_distribution}的熵的微分趋近于负无穷。
然而，人为加入一些$\Vmu$的噪声会使得这个界又有了意义。
% 628



\gls{MAP}推断作为特征提取器以及一种学习机制被广泛的应用在了\gls{DL}中。
在\gls{sparse_coding}模型中，它起到了关键作用。
% 628


我们回过头来看\ref{sec:sparse_coding}节中的\gls{sparse_coding}，\gls{sparse_coding}是一种在隐含节点上加上了鼓励稀疏的先验知识的\gls{linear_factor}。
一个常用的选择是可分解的拉普拉斯先验，表示为
\begin{align}
	p(h_i) = \frac{\lambda}{2} \exp(-\lambda\vert h_i\vert)
\end{align}
可见的节点是由一个线性变化加上噪音生成的\footnote{此处似乎有笔误，$\Vx$应为$\Vv$}:
\begin{align}
	p(\Vx\vert\Vh) = \CalN(\Vv;{\MW}\Vh + \Vb,\beta^{-1}{\MI})
\end{align}
% 628 end




计算或者表达$p(\Vh\vert\Vv)$太过困难。
每一对$h_i$， $h_j$变量都是$\Vv$的母节点。
这也意味着当$\Vv$是可观察时，\gls{graphical_models}包含了连接$h_i$和$h_j$的路径。
因此在$p(\Vh \vert\Vv)$中所有的隐含节点都包含在了一个巨大的\gls{clique}中。
如果模型是高斯的，那么这些关系可以通过协方差矩阵来高效的建模。
然而稀疏型先验使得模型并不是高斯。
% 629 head



$p(\Vx\vert\Vh)$的复杂性导致了似然函数的对数及其梯度也很难得到。
因此我们不能使用精确的\gls{MLE}来进行学习。
取而代之的是，我们通过\glssymbol{MAP}推断以及最大化由以$\Vh$为中心的\gls{dirac_distribution}所定义而成的\glssymbol{ELBO}来学习模型参数。
% 629


如果我们将训练集中的所有的$\Vh$向量拼在一起并且记为$\MH$，并将所有的$\Vv$向量拼起来组成矩阵$\MV$，那么\gls{sparse_coding}问题意味着最小化
\begin{align}
	J(\MH,\MW) = \sum_{i,j}^{}\vert H_{i,j}\vert 
	+ \sum_{i,j}^{}\Big(\MV - \MH \MW^{\top}\Big)^2_{i,j}
\end{align}
为了避免如极端小的$\MH$和极端大的$\MW$这样的病态的解，许多\gls{sparse_coding}的应用包含了权值衰减或者$\MH$的列的范数的限制。
% 629


我们可以通过交替迭代最小化$J$分别关于$\MH$和$\MW$的方式来最小化$J$。
两个子问题都是凸的。
事实上，关于$\MW$的最小化问题就是一个\gls{linear_regression}问题。
然而关于这两个变量同时最小化$J$的问题并不是凸的。
% 629


关于$\MH$的最小化问题需要某些特别设计的算法诸如特征符号搜索方法\citep{HonglakLee-2007}。
% 629


\section{变分推断和学习}
\label{variational_inference_and_learning}
% 629


我们已经说明过了为什么\glssymbol{ELBO}是$\log  p(\Vv;\Vtheta)$的一个下界， 如何将推断看做是 关于$q$分布最大化$\CalL$ 的过程以及如何将学习看做是关于参数$\Vtheta$最大化$\CalL$的过程。
我们也讲到了\glssymbol{EM}算法在给定了$q$分布的条件下进行学习，而\glssymbol{MAP}推断的算法则是学习一个$p(\Vh \vert \Vv)$的点估计而非推断整个完整的分布。
在这里我们介绍一些变分学习中更加通用的算法。
% 629


变分学习的核心思想就是我们通过选择给定的分布族中的一个$q$分布来最大化$\CalL$。
选择这个分布族的时候应该考虑到计算$\SetE_q \log p(\Vh,\Vv)$的简单性。
一个典型的方法就是添加一些假设诸如$q$可以分解。
% 630 head


一种常用的变分学习的方法是加入一些限制使得$q$是一个可以分解的分布：
\begin{align}
	q(\Vh\vert\Vv) = \prod_{i}^{}q(h_i \vert \Vv)
\end{align}
这被叫做是\firstgls{mean_field}方法。
一般来说，我们可以通过选择$q$分布的形式来选择任何\gls{graphical_models}的结构，通过选择变量之间的相互作用来决定近似程度的大小。
这种完全通用的\gls{graphical_models}方法叫做\firstgls{structured_variational_inference} \citep{Saul96}。
% 630 


变分方法的优点是我们不需要为分布$q$设定一个特定的参数化的形式。
我们设定它如何分解，而优化问题中决定了在这些分解限制下的最优的概率分布。
对离散型的隐含变量来说，这意味着我们使用了传统的优化技巧来优化描述$q$分布的有限个数的变量。
对连续性的变量来说，这意味着我们使用了一个叫做\firstgls{calculus_of_variations}的数学分支来解决对一个空间的函数的优化问题。
然后决定哪一个函数来表示$q$。
\gls{calculus_of_variations}是“变分学习”或者“变分推断”这些名字的来历，尽管当隐含变量是离散的时候\gls{calculus_of_variations}并没有用武之地。
当遇到连续的隐含变量的时候，\gls{calculus_of_variations}是一种很有用的工具，只需要设定分布$q$如何分解，而不需要过多的人工选择模型，比如尝试着设计一个特定的能够精确的近似原后验分布的$q$。
% 630 


因为$\CalL(\Vv,\Vtheta,q)$定义成$\log p(\Vv;\Vtheta) - D_{\text{KL}}(q(\Vh\vert\Vv)\Vert p(\Vh\vert\Vv;\Vtheta))$，我们可以认为关于$q$最大化$\CalL$的问题等价于最小化$D_{\text{KL}}(q(\Vh\vert\Vv)\Vert p(\Vh\vert\Vv))$。
在这种情况下，我们要用$q$来拟合$p$。
然而，我们并不是直接拟合一个近似，而是处理一个\gls{KL}的问题。
当我们使用\gls{MLE}来将数据拟合到模型的时候，我们最小化$D_{\text{KL}}(p_{\text{data}\Vert p_{\text{model}}})$。
如同图\ref{fig:chap3}中所示，这意味着\gls{MLE}促进模型在每一个数据达到更高概率的地方达到更高的概率，而基于优化的推断则促进了$q$在每一个真实后验分布概率较低的地方概率较小。
这两种方法都有各自的优点与缺点。
选择哪一种方法取决于在具体应用中哪一种性质更受偏好。
在基于优化的推断问题中，从计算角度考虑，我们选择使用$D_{\text{KL}}(q(\Vh\vert\Vv)\Vert p(\Vh\vert\Vv))$。
具体的说，计算$D_{\text{KL}}(q(\Vh\vert\Vv)\Vert p(\Vh\vert\Vv))$涉及到了计算$q$分布下的期望，所以通过将分布$q$设计的较为简单，我们可以简化求所需要的期望的计算过程。
另一个\gls{KL}的方向需要计算真实后验分布下的期望。
因为真实后验分布的形式是由模型的选择决定的，我们不能设计出一种能够精确计算$D_{\text{KL}}(p(\Vh\vert\Vv) \Vert q(\Vh\vert\Vv))$的开销较小的方法。
% 631 head




\subsection{离散隐含变量}
\label{sec:discrete_latent_variables}
% 631  19.4.1

关于离散型的隐含变量的变分推断相对来说更为直接。
我们定义一个分布$q$，通常$q$的每个因子都由一些离散状态的表格定义。
在最简单的情况中，$\Vh$是二元的并且我们做了\gls{mean_field}的假定，$q$可以根据每一个$h_i$分解。
在这种情况下，我们可以用一个向量$\hat{\Vh}$来参数化$q$分布，$\hat{\Vh}$的每一个元素都代表一个概率，即$q(h_i = 1\vert \Vv) = \hat{h}_i$。
% 631


在确定了如何表示$q$以后，我们简单的优化了它的参数。
在离散型的隐含变量模型中，这是一个标准的优化问题。
基本上$q$的选择可以通过任何的优化算法解决，比如说\gls{GD}。
% 631


这个优化问题是很高效的因为它在许多学习算法的内循环中出现。
为了追求速度，我们通常使用特殊设计的优化算法。
这些算法通常能够在极少的循环内解决一些小而简单的问题。
一个常见的选择是使用\gls{fixed_point_equation}，换句话说，就是对$\hat{h}_i$解决
\begin{align}
	\frac{\partial}{\partial \hat{h}_i}\CalL = 0
\end{align}
我们反复的更新$\hat{\Vh}$不同的元素直到收敛条件满足。
% 631


为了具体化这些描述，我们接下来会讲如何将变分推断应用到\firstgls{binary_sparse_coding}模型（这里我们所描述的模型是\citet{henniges2010binary}提出的，但是我们采用了传统的通用的\gls{mean_field}方法，而原文作者采用了一种特殊设计的算法）中。
推导过程在数学上非常详细，为希望完全了解变分推断细节的读者所准备。
而对于并不计划推导或者实现变分学习算法的读者来说，可以完全跳过，直接阅读下一节，这并不会导致重要概念的遗漏。
那些从事过\gls{binary_sparse_coding}研究的读者可以重新看一下\ref{sec:userful_properties_of_common_functions}节中的一些经常在概率模型中出现的有用的函数性质。
我们在推导过程中随意的使用了这些性质，并没有特别强调它们。
% 632  head


在\gls{binary_sparse_coding}模型中，输入$\Vv\in\SetR^n$，是由模型通过添加高斯噪音到$m$个或有或无的成分。
每一个成分可以是开或者关的通过对应的隐藏层节点$\Vh \in\{0,1\}^m$:
\begin{align}
	p(h_i = 1) = \sigma(b_i)
\end{align}
\begin{align}
	p(\Vv\vert\Vh) = \CalN(\Vv;\MW \Vh,{\Vbeta}^{-1})
\end{align}
其中$\Vb$是一个可以学习的偏置向量\footnote{此处似乎有笔误，公式里面漏了$\Vb$?}，$\MW$是一个可以学习的权值矩阵，${\Vbeta}$是一个可以学习的对角的精度矩阵。
% 632


使用\gls{MLE}来训练这样一个模型需要对参数进行求导。
我们考虑对其中一个偏置进行求导的过程：
\begin{align}
		& \frac{\partial}{\partial b_i} \log p(\Vv) \\
		= &  \frac{\frac{\partial}{\partial b_i} p(\Vv)}{p(\Vv)}\\
		= & \frac{\frac{\partial}{\partial b_i} \sum_{\Vh}^{} p(\Vh,\Vv)}{p(\Vv)}\\
		= &  \frac{\frac{\partial}{\partial b_i} \sum_{\Vh}^{} p(\Vh) p(\Vv\vert \Vh)  }{p(\Vv)}\\
		= &  \frac{ \sum_{\Vh}^{}  p(\Vv\vert \Vh) \frac{\partial}{\partial b_i} p(\Vh) }{p(\Vv)}\\
		= &  \sum_{\Vh}^{}  p(\Vh\vert \Vv) \frac{  \frac{\partial}{\partial b_i} p(\Vh) }{p(\Vh)}\\
		= & \SetE_{\Vh\sim p(\Vh\vert\Vv)} \frac{\partial}{\partial b_i}\log p(\Vh)
\end{align}
% 632 end

这需要计算$p(\Vh\vert\Vv)$下的期望。
不幸的是，$p(\Vh\vert\Vv)$是一个很复杂的分布。
$p(\Vh\vert\Vv)$和$p(\Vh,\Vv)$的图结构参见\ref{fig:bsc}。
隐含节点的后验分布对应的是完全图，所以相对于暴力算法，消元算法并不能有助于提高计算所需要的期望的效率。
% 633 head


\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
	\centerline{\includegraphics{Chapter19/figures/bsc}}
\fi
	\caption{TODO}
	\label{fig:bsc}
\end{figure}



取而代之的是，我们可以应用变分推断和变分学习来解决这个难点。
% 633


我们可以做一个\gls{mean_field}的假设：
\begin{align}
	q(\Vh\vert\Vv) = \prod_{i}^{}q(h_i\vert\Vv)
\end{align}
% 633


\gls{binary_sparse_coding}中的隐含变量是二值的，所以为了表示可分解的$q$我们假设模型为$m$个\gls{bernoulli_distribution}$q(h_i\vert\Vv)$。
表示\gls{bernoulli_distribution}的一种常见的方法是一个概率向量$\hat{\Vh}$，满足$q(h_i\vert\Vv) = \hat{h}_i$。
为了避免计算中的误差，比如说计算$\log \hat{h}_i$的时候，我们对$\hat{h}_i$添加一个约束，即$\hat{h}_i$不等于0或者1。
% 633 


我们将会看到变分推断方程理论上永远不会赋予$\hat{h}_i\ $ 0或者1。
然而在软件实现过程中，机器的舍入误差会导致0或者1的值。
在\gls{binary_sparse_coding}的实现中，我们希望使用一个没有限制的变分参数向量$\Vz$以及
通过关系$\hat{\Vh} = \sigma(\Vz)$来获得$\Vh$。
因此我们可以放心的在计算机上计算$\log \hat{h}_i$通过使用关系式$\log \sigma(z_i) = -\zeta(-z_i)$来建立sigmoid和softplus的关系。
% 633


在开始\gls{binary_sparse_coding}模型的推导时，我们首先说明了\gls{mean_field}的近似可以使得学习的过程更加简单。
% 633 end  


\glssymbol{ELBO}可以表示为
\begin{align}
\label{eqn:1936}
& \CalL(\Vv,\Vtheta,q)\\
 = & \SetE_{\Vh\sim q}[\log p(\Vh,\Vv)] + H(q)\\
 = & \SetE_{\Vh\sim q}[\log p(\Vh) + \log p(\Vv\vert\Vh) - \log q(\Vh\vert\Vv)]\\
= & \SetE_{\Vh\sim q}\Big[\sum_{i=1}^{m}\log p(h_i) + \sum_{i=1}^{n} \log p(v_i\vert\Vh) - \sum_{i=1}^{m}\log q(h_i\vert\Vv)\Big]\\
= &  \sum_{i=1}^{m}\Big[\hat{h}_i(\log \sigma(b_i) - \log \hat{h}_i) + (1 - \hat{h}_i)(\log \sigma(-b_i) - \log (1-\hat{h}_i))\Big] \\
& +  \SetE_{\Vh\sim q} \Bigg[ \sum_{i=1}^{n}\log \sqrt{\frac{\beta_i}{2\pi}}\exp(-\frac{\beta_i}{2}(v_i - \MW_{i,:}\Vh)^2)\Bigg] \\
= &  \sum_{i=1}^{m}\Big[\hat{h}_i(\log \sigma(b_i) - \log \hat{h}_i) + (1 - \hat{h}_i)(\log \sigma(-b_i) - \log (1-\hat{h}_i))\Big] \\
& + \frac{1}{2} \sum_{i=1}^{n} \Bigg[ \log {\frac{\beta_i}{2\pi}} - \beta_i \Bigg(v_i^2 - 2 v_i \MW_{i,:}\hat{\Vh} + \sum_{j}\Big[W^2_{i,j}\hat{h}_j + \sum_{k \neq j}W_{i,j}W_{i,k}\hat{h}_j\hat{h}_k \Big] \Bigg) \Bigg] 
\end{align}
% 634 head 


尽管从美学观点这些方程有些不尽如人意。
他们展示了$\CalL$可以被表示为少数的简单的代数运算。
因此\glssymbol{ELBO}是易于处理的。
我们可以把$\CalL$看作是难以处理的似然函数的对数。
% 634  head 


原则上说，我们可以使用关于$\Vv$和$\Vh$的梯度下降。
这会成为一个完美的组合的推断学习算法。
但是，由于两个原因，我们往往不这么做。
第一点，对每一个$\Vv$我们需要存储$\hat{\Vh}$。
我们通常更加偏向于那些不需要为每一个样本都准备内存的算法。
如果我们需要为每一个样本都存储一个动态更新的向量，很难使得算法能够处理好几亿的样本。
第二个原因就是为了能够识别$\Vv$的内容，我们希望能够有能力快速提取特征$\hat{\Vh}$。
在实际应用下，我们需要在有限时间内计算出$\hat{\Vh}$。
% 634


由于以上两个原因，我们通常不会采用\gls{GD}来计算\gls{mean_field}的参数$\hat{\Vh}$。
取而代之的是，我们使用\gls{fixed_point_equation}来快速估计他们。
% 634


\gls{fixed_point_equation}的核心思想是我们寻找一个关于$\Vh$的局部最大值，满足$\nabla_{\Vh}\CalL(\Vv,\Vtheta,\hat{\Vh}) = 0$。
我们无法同时高效的计算所有的$\hat{\Vh}$的元素。
然而，我们可以解决单个变量的问题：
\begin{align}
\label{eqn:1937}
\frac{\partial}{\partial \hat{h}_i} \CalL(\Vv,\Vtheta,\hat{\Vh}) = 0 
\end{align}
% 634 end  


我们可以迭代的将这个解应用到$i = 1,\ldots,m$，然后重复这个循环直到我们满足了收敛标准。
常见的收敛标准包含了当整个循环所改进的$\CalL$不超过预设的容忍的量时停止，或者是循环中改变的$\hat{\Vh}$不超过某个值的时候停止。
% 635  head

在很多不同的模型中，迭代的\gls{mean_field}\gls{fixed_point_equation}是一种能够提供快速变分推断的通用的算法。
为了使他更加具体化，我们详细的讲一下如何推导出\gls{binary_sparse_coding}模型的更新过程。
% 635


首先，我们给出了对$\hat{h}_i$的导数的表达式。
为了得到这个表达式，我们将方程~\eqref{eqn:1936}代入到方程~\eqref{eqn:1937}的左边：
\begin{align}
\label{eqn:1943}
& \frac{\partial}{\partial \hat{h}_i} \CalL (\Vv,\Vtheta,\hat{\Vh})    \\
= & \frac{\partial}{\partial \hat{h}_i} \Bigg[\sum_{j=1}^{m}\Big[\hat{h}_j (\log \sigma(b_j) - \log \hat{h}_j) + (1 - \hat{h}_j)(\log \sigma(-b_j) - \log (1-\hat{h}_j))\Big] \\
& + \frac{1}{2} \sum_{j=1}^{n} \Bigg[ \log {\frac{\beta_j}{2\pi}} - \beta_j \Bigg(v_j^2 - 2 v_j \MW_{j,:}\hat{\Vh} + \sum_{k}\Bigg[W^2_{j,k}\hat{h}_k + \sum_{l \neq k}W_{j,k}W_{j,l}\hat{h}_k\hat{h}_l \Bigg] \Bigg) \Bigg]\Bigg] \\
= & \log \sigma(b_i) - \log \hat{h}_i - 1 + \log (1 - \hat{h}_i ) + 1 - \log \sigma (- b_i ) \\
 & + \sum_{j=1}^{n} \Bigg[\beta_j \Bigg(v_j W_{j,i} - \frac{1}{2} W_{j,i}^2 - \sum_{k \neq i} \MW_{j,k}\MW_{j,i} \hat{h}_k\Bigg) \Bigg]\\
 = & b_i - \log \hat{h}_i + \log (1 - \hat{h}_i ) + \Vv^{\top} {\Vbeta} \MW_{:,i} - \frac{1}{2} \MW_{:,i}^{\top} {\Vbeta}\MW_{:,i} -\sum_{j\neq i}\MW^{\top}_{:,j}{\Vbeta}\MW_{:,i}\hat{h}_j 
\end{align}
% 635  

为了应用固定点更新的推理规则，我们通过令方程~\eqref{eqn:1943}等于0来解$\hat{h}_i$：

\begin{align}
\label{eqn:1944}
\hat{h}_i = \sigma\Bigg(b_i + \Vv^{\top} {\Vbeta} \MW_{:,i} - \frac{1}{2} \MW_{:,i}^{\top} {\Vbeta} \MW_{:,i} - \sum_{j \neq i }  \MW_{:,j}^{\top} {\Vbeta}  \MW_{:,i} \hat{h}_j \Bigg)
\end{align}
% 635 end

此时，我们可以发现\gls{graphical_models}中\gls{RNN}和推断之间存在着紧密的联系。
具体的说，\gls{mean_field}\gls{fixed_point_equation}定义了一个\gls{RNN}。
这个神经网络的任务就是完成推断。
我们已经从模型描述角度介绍了如何推导这个网络，但是直接训练这个推断网络也是可行的。
有关这种思路的一些想法在第\ref{chap:deep_generative_models}章中有所描述。
% 636  head 

<bad>
在\gls{binary_sparse_coding}模型中，我们可以发现方程~\eqref{eqn:1944}中描述的循环网络包含了根据相邻的变化着的隐藏层结点值来反复更新当前隐藏层结点的操作。
输入层通常给隐藏层结点发送一个固定的信息$\Vv^{\top}\beta\MW$，然而隐藏层结点不断地更新互相传送的信息。
具体的说，当$\hat{h}_i$和$\hat{h}_j$两个单元的权重向量对准时，他们会产生相互抑制。
这也是一种形式的竞争－－－两个共同理解输入的隐层结点之间，只有一个解释的更好的才能继续保持活跃。
在\gls{binary_sparse_coding}的后杨分布中，\gls{mean_field}近似为了捕获到更多的\gls{explaining_away}作用，产生了这种竞争。
事实上，\gls{explaining_away}效应会导致一个多峰值的后验分布，所以我们如果从后验分布中采样，一些样本只有一个结点是活跃的，其它的样本在其它的结点活跃，只有很少的样本能够所有的结点都处于活跃状态。
不幸的是，\gls{explaining_away}作用无法通过\gls{mean_field}中可分解的$q$分布来建模，因此建模时\gls{mean_field}近似只能选择一个峰值。
这个现象的一个例子可以参考图~\ref{fig:}。
% 636




我们将方程~\eqref{eqn:1944}重写成等价的形式来揭示一些深层的含义：
\begin{align}
\hat{h}_i = \sigma\Bigg(b_i + \Big(\Vv - \sum_{j\neq i} \MW_{:,j}\hat{h}_j\Big)^{\top} {\Vbeta}\MW_{:,i} - \frac{1}{2} \MW_{:,i}^{\top} {\Vbeta} \MW_{:,i}\Bigg) 
\end{align}
在这种新的形式中，我们可以将每一步的输入看成$\Vv - \sum_{j\neq i} \MW_{:,j}\hat{h}_j$而不是$\Vv$。
因此，我们可以把第$i$个单元视作给定其它单元的编码时编码$\Vv$中的剩余误差。
由此我们可以将\gls{sparse_coding}视作是一个迭代的\gls{AE}，反复的将输入的编码解码，试图逐渐减小重构误差。
% 636


在这个例子中，我们已经推导出了每一次更新单个结点的更新规则。
接下来我们考虑如何同时更新许多结点。
某些\gls{graphical_models}，比如\glssymbol{DBM}，我们可以同时更新$\hat{\Vh}$中的许多元素。
不幸的是，\gls{binary_sparse_coding}并不适用这种块更新。
取而代之的是，我们使用一种称为是\firstgls{damping}的启发式的技巧来实现块更新。
在\gls{damping}方法中，对$\hat{\Vh}$中的每一个元素我们都可以解出最优值，然后对于所有的值都在这个方向上移动一小步。
这个方法不能保证每一步都能增加$\CalL$，但是对于许多模型来说却很有效。
关于在\gls{message_passing}算法中如何选择同步程度以及使用\gls{damping}策略可以参考\citet{koller-book2009} 。
 % 637 head 




\subsection{\gls{calculus_of_variations}}
\label{sec:calculus_of_variations}
% p 637 head   19.4.2 

在继续描述变分学习之前，我们有必要简单的介绍一种变分学习中的重要的数学工具：\firstgls{calculus_of_variations}。
% p 637


许多\gls{ML}的技巧是基于寻找一个输入向量$\Vtheta\in\SetR^n$来最小化函数$J(\Vtheta)$，使得它取到最小值。
这个步骤可以利用多元微积分以及线性代数的知识找到满足$\nabla_{\Vtheta} J(\Vtheta) = 0$的临界点来完成。
在某些情况下，我们希望能够找出一个最优的函数$f(\Vx)$，比如当我们希望找到一些随机变量的概率密度函数的时候。
\gls{calculus_of_variations}能够让我们完成这个目标。
% p 637



$f$函数的函数被称为是\firstgls{functional}$J[f]$。
正如我们许多情况下对一个函数求关于以向量为变量的偏导数一样，我们可以使用\firstgls{functional_derivative}，即对任意一个给定的$\Vx$，对一个\gls{functional}$J[f]$求关于函数$f(\Vx)$的导数，这也被称为\firstgls{variational_derivative}。
\gls{functional}$J$的关于函数$f$在点$\Vx$处的\gls{functional_derivative}被记作$\frac{\delta}{\delta f(x)}J$。
% p 637



完整正式的\gls{functional_derivative}的推导不在本书的范围之内。
为了满足我们的目标，讲述可微分函数$f(\Vx)$以及带有连续导数的可微分函数$g(y,\Vx)$就足够了：
\begin{align}
	\frac{\delta}{\delta f(\Vx)} \int g(f(\Vx),\Vx)d\Vx = \frac{\partial}{\partial y}g(f(\Vx),\Vx)
\end{align}
为了使上述的关系式更加的形象，我们可以把$f(\Vx)$看做是一个有着无穷多元素的向量。
在这里（看做是一个不完全的介绍），这种关系式中描述的\gls{functional_derivative}和向量$\Vtheta\in\SetR^n$的导数相同：
\begin{align}
	\frac{\partial}{\partial \theta_i}\sum_{j}^{}g(\theta_j,j)
	= \frac{\partial}{\partial \theta_i}g(\theta_i,i)
\end{align}
在其他\gls{ML}的文献中的许多结果是利用了更为通用的\firstgls{euler_lagrange_eqn}，
它能够使得$g$不仅依赖于$f$的导数而且也依赖于$f$的值。
但是本书中我们不需要把完整的讲述这个结果。
% p 637  end


为了优化某个函数关于一个向量，我们求出了这个函数关于这个向量的梯度，然后找一个梯度的每一个元素都为0的点。
类似的，我们可以通过寻找一个函数使得\gls{functional_derivative}的每个点都等于0从而来优化一个\gls{functional}。
% p 638 head


下面讲一个这个过程是如何工作的，我们考虑寻找一个定义在$x\in\SetR$的有最大微分熵的概率密度函数。
我们回过头来看一下一个概率分布$p(x)$的熵，定义如下：
\begin{align}
	H[p] = - \SetE_x \log p(x)
\end{align}
对于连续的值，这个期望可以看成是一个积分：
\begin{align}
	H[p] = - \int p(x) \log p(x) dx
\end{align}
% p 638


我们不能简单的关于函数$p(x)$最大化$H[p]$，因为那样的化结果可能不是一个概率分布。
为了解决这个问题，我们需要使用一个拉格朗日乘子来添加一个$p(x)$积分值为1的约束。
此外，当方差增大的时候，熵也会无限制的增加。
因此，寻找哪一个分布有最大熵这个问题是没有意义的。
但是，在给定固定的方差$\sigma^2$的时候，我们可以寻找一个最大熵的分布。
最后，这个问题还是无法确定因为在不改变熵的条件下一个分布可以被随意的改变。
为了获得一个唯一的解，我们再加一个约束：分布的均值必须为$\mu$。
那么这个问题的拉格朗日\gls{functional}可以被写成
\begin{align}
\label{eqn:1950}
&		\CalL[p] =  \lambda_1 \Big(\int p(x)dx - 1\Big)  + \lambda_2 (\SetE[x] - \mu) +  \lambda_3 (\SetE[(x-\mu)^2] - \sigma^2)  + H[p]\\
& =  \int \Big(\lambda_1 p(x) + \lambda_2 p(x)x + \lambda_3 p(x)(x-\mu)^2 - p(x)\log p(x) \Big)dx - \lambda_1 - \mu \lambda_2 - \sigma^2\lambda_3
\end{align}
% p 638   


为了关于$p$最小化拉格朗日乘子，我们令\gls{functional_derivative}等于0：
\begin{align}
\label{eqn:1952}
	\forall x,\ \  \frac{\delta}{\delta p(x)} \CalL = \lambda_1 + \lambda_2 x + \lambda_3(x-\mu)^2 - 1 - \log p(x) = 0 
\end{align}
% p 638  end 


这个条件告诉我们$p(x)$的\gls{functional}形式。
通过代数运算重组上述方程，我们可以得到
\begin{align}
\label{eqn:1953}
	p(x) = \exp\big(\lambda_1 + \lambda_2 x + \lambda_3 (x-\mu)^2  - 1\big)
\end{align}
% p 638 end 

我们并没有直接假设$p(x)$取这种形式，而是通过最小化这个\gls{functional}从理论上得到了这个$p(x)$的表达式。
为了完成这个优化问题，我们需要选择$\lambda$的值来确保所有的约束都能够满足。
我们有很大的选择$\lambda$的自由。
因为只要约束满足，拉格朗日关于$\lambda$的梯度为0。
为了满足所有的约束，我们可以令$\lambda_1 = 1 - \log \sigma\sqrt{2\pi}$,$\lambda_2 = 0$, $\lambda_3 = - \frac{1}{2\sigma^2}$，从而取到
\begin{align}
\label{eqn:1954}
	p(x) = \CalN(x;\mu,\sigma^2)
\end{align}
这也是当我们不知道真实的分布的时候总是使用正态分布的原因。
因为正态分布拥有最大的熵，<bad>我们通过这个假定来保证了最小可能量的结构。
% 639 head



当寻找熵的拉格朗日\gls{functional}的临界点并且给定一个固定的方差的时候，我们只能找到一个对应最大熵的临界点。
那是否存在一个最小化熵的概率分布函数呢？
为什么我们无法发现第二个最小值的临界点呢？
原因是没有一个特定的函数能够达到最小的熵值。
当函数把越多的概率密度加到$x = \mu + \sigma$和$x = \mu - \sigma$两个点上和越少的概率密度到其他点上时，他们的熵值会减少，而方差却不变。
然而任何把所有的权重都放在这两点的函数的积分并不为1。
所以不存在一个最小熵的概率分布函数，就像不存在一个最小的正实数一样。
然而，我们发现存在一个概率分布的收敛的序列，直到把权重都放在两个点上。
这种情况能够退化为混合\gls{dirac_distribution}。
因为\gls{dirac_distribution}并不是一个单独的概率分布，所以\gls{dirac_distribution}或者混合\gls{dirac_distribution}并不能对应函数空间的一个点。
所以对我们来说，当寻找一个\gls{functional_derivative}为0的函数空间的点的时候，这些分布是不可见的。
这就是这种方法的局限之处。
像\gls{dirac_distribution}这样的分布可以通过其他方法被找到，比如可以被猜到，然后证明它是满足条件的。
% 639 



\subsection{连续型隐含变量}
\label{sec:continuous_latent_variables}
% 639 end            19.4.3 


当我们的\gls{graphical_models}包含了连续型隐含变量的时候，我们仍然可以通过最大化$\CalL$进行变分推断和学习。
然而，我们需要使用\gls{calculus_of_variations}来实现关于$q(\Vh\vert\Vv)$最大化$\CalL$。
% 639 end  


% 640 head  
在大多数情况下，研究者并不需要解决任何的\gls{calculus_of_variations}的问题。
取而代之的是，\gls{mean_field}固定点迭代有一种通用的方程。
如果我们做了\gls{mean_field}的近似：
\begin{align}
\label{eqn:1955}
	q(\Vh\vert\Vv) = \prod_i q(h_i \vert\Vv)
\end{align}
并且对任何的$j\neq i$固定了$q(h_j\vert\Vv)$，那么只需要满足$p$中任何联合分布中的变量不为0,我们就可以通过归一化下面这个未归一的分布
\begin{align}
	\label{eqn:1956}
	\tilde{q}(h_i \vert\Vv) = \exp
	(\SetE_{h_{-i}\sim q(h_{-i}\vert\Vv)}
	\log \tilde{p}(\Vv,\Vh))
\end{align}
来得到最优的$q(h_i\vert\Vv)$。
在这个方程中计算期望就能得到一个$q(h_i\vert\Vv)$的正确表达式。
我们只有在希望提出一种新形式的变分学习算法时才需要直接推导$q$的函数形式。
方程~\eqref{eqn:1956}给出了适用于任何概率模型的\gls{mean_field}近似。
% 640  




% 640  
方程~\eqref{eqn:1956}是一个\gls{fixed_point_equation}，对每一个$i$它都被迭代的反复使用直到收敛。
然而，它还包含着更多的信息。
我们发现这种\gls{functional}定义的问题的最优解是存在的，无论我们是否能够通过\gls{fixed_point_equation}来解出它。
这意味着我们可以把一些值当成参数，然后通过优化算法来解决这个问题。


% 640  
我们拿一个简单的概率模型作为一个例子，其中隐含变量满足$\Vh\in\SetR^2$，可视的变量只有一个$v$。
假设$p(\Vh) = \CalN(\Vh;0,\MI)$以及$p(v\vert\Vh) = \CalN(v;\Vw^{\top}\Vh;1)$，我们可以通过把$\Vh$积掉来简化这个模型，结果是关于$v$的高斯分布。
这个模型本身并不有趣。
为了说明\gls{calculus_of_variations}如何应用在概率建模，我们构造了这个模型。



% 640  end
忽略归一化常数的时候，真实的后验分布可以给出：
\begin{align}
	\label{eqn:1957}
   & p(\Vh\vert\Vv)\\
 \propto & p(\Vh, \Vv)\\
 = & p(h_1) p(h_2) p(\Vv\vert\Vh)\\
 \propto & \exp(-\frac{1}{2} [h_1^2 + h_2^2 + (v-h_1w_1 - h_2w_2)^2])\\
 = & \exp(-\frac{1}{2}[h_1^2 + h_2^2 + v^2 + h_1^2w_1^2 + h_2^2w_2^2 - 2vh_2w_2 - 2vh_2w_2 + 2h_1w_1h_2w_2])
\end{align}
在上式中，我们发现由于带有$h_1,h_2$乘积的项的存在，真实的后验并不能将$h_1,h_2$分开。
% 641 head 



展开方程~\eqref{eqn:1956}，我们可以得到
\begin{align}
\label{eqn:1962}
& \tilde{q}(h_1\vert\Vv)\\ = & \exp(\SetE_{h_2\sim q(h_2\vert\Vv)} \log \tilde{p}(\Vv,\Vh))\\
= & \exp\Big( -\frac{1}{2} \SetE_{h_2\sim q(h_2\vert\Vv)} [h_1^2 + h_2^2 + v^2 + h_1^2w_1^2 + h_2^2w_2^2 \\
& -2vh_1w_1 - 2vh_2w_2 + 2h_1w_1h_2w_2]\Big)	
\end{align}
% 641 


% 641 
从这里，我们可以发现其中我们只需要从$q(h_2\vert \Vv)$中获得两个值：$\SetE_{h_2\sim q(h\vert\Vv)}[h_2]$和$\SetE_{h_2\sim q(h\vert\Vv)}[h_2^2]$。
把这两项记作$\langle h_2 \rangle$和$\langle h_2^2 \rangle$，我们可以得到：
\begin{align}
\label{eqn:1966}
\tilde{q}(h_1\vert\Vv) = & \exp(-\frac{1}{2} [h_1^2 + \langle h_2^2 \rangle  + v^2 + h_1^2w_1^2 + \langle h_2^2 \rangle w_2^2 
\\ &	-2vh_1w_1 - 2v\langle h_2 \rangle w_2 + 2h_1w_1\langle h_2 \rangle w_2])	
\end{align}
% 641 


从这里，我们可以发现$\tilde{q}$形式满足高斯分布。
因此，我们可以得到$q(\Vh\vert\Vv) = \CalN(\Vh;\Vmu,\Vbeta^{-1})$，其中$\Vmu$和对角的$\Vbeta$是变分参数，我们可以使用任何方法来优化它。
有必要说明一下，我们并没有假设$q$是一个高斯分布，这个高斯是使用\gls{calculus_of_variations}来最大化$q$关于$\CalL$\footnote{此处似乎有笔误。}推导出的。
在不同的模型上应用相同的方法可能会得到不同形式的$q$分布。
% 641

当然，上述模型只是为了说明情况的一个简单例子。
\gls{DL}中关于变分学习中连续变量的实际应用可以参考\citet{Goodfeli-et-al-TPAMI-Deep-PrePrint-2013-small}。
% 641



\subsection{学习和推断之间的相互作用}
\label{sec:interactions_between_learning_and_inference}
% 641 end  19.4.4  


%使用近似推理作为学习算法的一部分影响学习过程，反过来这也影响推理算法的准确性。
在学习算法中使用近似推理会影响学习的过程，反过来这也会影响推理算法的准确性。
% 641 end


具体来说，训练算法倾向于以使得近似推理算法中的近似假设变得更加真实的方向来适应模型。<bad>
当训练参数时，变分学习增加
\begin{align}
\label{eqn:1968}
	\SetE_{\Vh \sim q}\log p(\Vv,\Vh)
\end{align}
% 642 

对于一个特定的$\Vv$，对于$q(\Vh \vert \Vv)$中概率很大的$\Vh$它增加了$p(\Vh\vert\Vv)$，对于$q(\Vh \vert \Vv)$中概率很小的$\Vh$它减小了$p(\Vh\vert\Vv)$。
% 642

这种行为使得我们做的近似假设变得合理。 %这种行为使我们的近似假设成为自我实现。
<bad> 如果我们用单峰值的模型近似后验分布，我们将获得一个真实后验的模型，该模型比我们通过使用精确推理训练模型获得的模型更接近单峰。
% 642


因此，估计由于变分近似所产生的对模型的坏处是很困难的。
存在几种估计$\log p(\Vv)$的方式。
通常我们在训练模型之后估计$\log p(\Vv;\Vtheta)$，然后发现它和$\CalL(\Vv,\Vtheta,q)$的差距是很小的。
从这里我们可以发现，对于特定的从学习过程中获得的$\Vtheta$来说，变分近似普遍是很准确的。
然而我们无法直接得到变分近似普遍很准确或者变分近似不会对学习过程产生任何负面影响这样的结论。
为了准确衡量变分近似带来的危害，我们需要知道$\Vtheta^* = \max_{\Vtheta} \log p(\Vv;\Vtheta)$。
$\CalL(\Vv,\Vtheta,q)\approx \log p(\Vv;\Vtheta)$和$\log p(\Vv;\Vtheta)\ll \log p(\Vv;\Vtheta^*)$同时成立是有可能的。
如果存在$\max_q \CalL(\Vv,\Vtheta^*,q)\ll \log p(\Vv;\Vtheta^*)$，即在$\Vtheta^*$点处后验分布太过复杂使得$q$分布族无法准确描述，则我们无法学习到一个$\Vtheta$。
这样的一类问题是很难发现的，因为只有在我们有一个能够找到$\Vtheta^*$的超级学习算法时，才能进行上述的比较。
% 642



\section{\gls{learned}近似推断}
\label{sec:learned_approximate_inference}
% 642  end    19.5

我们已经看到了推断可以被视作是一个增加函数$\CalL$的值的一个优化过程。
显式的通过迭代方法比如\gls{fixed_point_equation}或者基于梯度的优化算法来执行优化的过程通常是代价昂贵且耗时巨大的。
<bad>具体的说，我们可以将优化过程视作一个将输入$\Vv$投影到一个近似分布$q^* = \arg\max_q\  \CalL(\Vv,q)$的函数$f$。
<bad>一旦我们将多步的迭代优化过程看作是一个函数，我们可以用一个近似$\hat{f}(\Vv;{\Vtheta})$的\gls{NN}来近似。


\subsection{\gls{wake_sleep}}
\label{sec:wake_sleep}
% 643 head 19.5.1

训练一个可以用$\Vv$来推断$\Vh$的模型的一个主要的难点在于我们没有一个有监督的训练集来训练模型。
给定一个$\Vv$，我们无法获知一个合适的$\Vh$。
从$\Vv$到$\Vh$的映射依赖于模型类型的选择，并且在学习过程中随着${\Vtheta}$的改变而变化。
\firstgls{wake_sleep}算法\citep{Hinton95,Frey96}通过从模型分布中抽取$\Vv$和$\Vh$样本来解决这个问题。
例如，在\gls{directed_model}中，这可以通过执行从$\Vh$开始并在$\Vv$结束的\gls{ancestral_sampling}来高效地完成。
然后推断网络可以被训练来执行反向的映射：预测哪一个$\Vh$产生了当前的$\Vv$。
<bad>这种方法的主要缺点是我们将只能够训练推理网络在模型下具有高概率的v值。
在学习早期，模型分布将不像数据分布，因此推理网络将不具有学习类似数据的样本的机会。
% 643


<bad>在\ref{sec:stochastic_maximum_likelihood_and_contrastive_divergence}节中，我们看到睡眠在人类和动物中的作用的一个可能的解释是，梦想可以提供\gls{monte_carlo}训练算法用于近似\gls{undirected_model}的对数\gls{partition_function}的负梯度的负相位样本。
生物作梦的另一个可能的解释是它提供来自$p(\Vh,\Vv)$的样本，这可以用于训练推理网络在给定$\Vv$的情况下预测$\Vh$。
在某些意义上，这种解释比\gls{partition_function}的解释更令人满意。
如果\gls{monte_carlo}算法仅使用梯度的正相位进行几个步骤，然后仅对梯级的负相位进行几个步骤，那么他们的结果不会很好。
人类和动物通常醒来连续几个小时，然后睡着连续几个小时。
这个时间表如何支持\gls{undirected_model}的\gls{monte_carlo}训练尚不清楚。
然而，基于最大化$\CalL$的学习算法可以通过长时间调整改进$q$和长期调整${\Vtheta}$来实现。
如果生物作梦的作用是训练网络来预测$q$，那么这解释了动物如何能够保持清醒几个小时（它们清醒的时间越长，$\CalL$和$\log p(\Vv)$之间的差距越大， 但是$\CalL$将保持下限）并且睡眠几个小时（生成模型本身在睡眠期间不被修改）， 而不损害它们的内部模型。
当然，这些想法纯粹是猜测性的，没有任何坚实的证据表明梦想实现了这些目标之一。
梦想也可以服务\gls{RL}而不是概率建模，通过从动物的过渡模型采样合成经验，从来训练动物的策略。
也许睡眠可以服务于一些机器学习社区尚未发现的其他目的。
% 644 head



\subsection{\gls{learned}推断的其它形式}
\label{sec:other_forms_of_learned_inference}
% 644    19.5.2 

这种\gls{learned}近似推断策略已经被应用到了其它模型中。
\citep{Salakhutdinov+Larochelle-2010}证明了在\gls{learned}推断网络中的单一路径相比于在\gls{DBM}中迭代\gls{mean_field}\gls{fixed_point_equation}能够得到更快的推断。
训练过程基于运行推理网络，然后应用\gls{mean_field}的一步来改进其估计，并训练推理网络来输出这个精细的估计而不是其原始估计。
% 644


我们已经在\ref{sec:predictive_sparse_decomposition}节中已经看到，预测性的稀疏分解模型训练浅层的\gls{encoder}网络以预测输入的\gls{sparse_coding}。
这可以被看作是\gls{AE}和\gls{sparse_coding}之间的混合。
为模型设计概率语义是可能的，其中\gls{encoder}可以被视为执行\gls{learned}近似\gls{MAP}推断。
由于其浅层的\gls{encoder}，PSD不能实现我们在\gls{mean_field}推理中看到的单元之间的那种竞争。
然而，该问题可以通过训练深度\gls{encoder}来执行\gls{learned}近似推理来补救，如ISTA技术\citep{Gregor+LeCun-ICML2010}。
% 644


近来\gls{learned}近似推理已经成为了\gls{VAE}形式的\gls{generative_model}中的主要方法之一\citep{Kingma+Welling-ICLR2014,Rezende-et-al-ICML2014}。
在这种优美的方法中，不需要为推断网络构造显式的目标。
反之，推断网络被用来定义$\CalL$，然后调整推断网络的参数来增大$\CalL$。这种模型在\ref{sec:variational_autoencoders}节中详细描述。
% 644

我们可以使用近似推理来训练和使用大量的模型。
许多模型将在下一章中被描述。
% 644















